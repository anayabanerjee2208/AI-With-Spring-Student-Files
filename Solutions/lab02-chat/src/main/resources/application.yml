spring:
  application.name: Lab02 Chat
  main.web-application-type: none     # Do not start a web server.

  ai:
    retry:
      max-attempts: 1           # Maximum number of retry attempts.
      on-client-errors: false   # Do not retry 4xx client error codes.

    model.chat: none            # Disable all chat models by default. Models will be explicitly enabled below.

---
spring:
  config.activate.on-profile: ollama

  # TODO-04: Set the spring.application.name to "Lab02 Chat Ollama" or something similar.
  # Set spring.main.web-application-type to none to run as a non-web application.
  # Set spring.ai.retry.max-attempts to 1 to fail fast.
  # Set spring.ai.retry.on-client-errors to false since there is typically no point in retrying such a request.
  # Set spring.ai.model.chat to ollama to tell SpringAI which autoconfigure class to use.
  # Set spring.ai.ollama.base-url to http://localhost:11434, unless your container is running on a different URL.

  application.name: Lab02 Chat Ollama
  ai:
    model.chat: ollama
    ollama:
      base-url: http://localhost:11434  # Default base URL when you run Ollama from Docker
      chat:
        options:
          model: mistral
---
spring:
  config.activate.on-profile: aws

  # TODO-04: Set the spring.application.name to "Lab02 Chat AWS" or something similar.
  # Set spring.main.web-application-type to none to run as a non-web application.
  # Set spring.ai.retry.max-attempts to 1 to fail fast.
  # Set spring.ai.retry.on-client-errors to false since there is typically no point in retrying such a request.
  # Set spring.ai.model.chat to bedrock-converse to tell SpringAI which autoconfigure class to use.
  # Set spring.ai.bedrock.aws.region to "us-west-2", or whichever region you have enabled the model in.
  # Set spring.ai.bedrock.converse.chat.options.model to "us.amazon.nova-micro-v1:0" or equivalent (make sure it is enabled)

  application.name: Lab02 Chat AWS
  ai:
    model.chat: bedrock-converse
    bedrock:
      aws.region: us-west-2 # Adjust as needed.
      converse:
        chat:
          options:
            # model: us.amazon.nova-micro-v1:0  # Adjust as needed.
            model: anthropic.claude-3-5-haiku-20241022-v1:0

---
spring:
  config.activate.on-profile: openai

  # TODO-04: Set the spring.application.name to "Lab02 Chat OpenAI" or something similar.
  # Set spring.main.web-application-type to none to run as a non-web application.
  # Set spring.ai.retry.max-attempts to 1 to fail fast.
  # Set spring.ai.retry.on-client-errors to false since there is typically no point in retrying such a request.
  # Set spring.ai.model.chat to openai to tell SpringAI which autoconfigure class to use.
  application.name: Lab02 Chat OpenAI
  ai:
    model.chat: openai
    openai:
      api-key: NEVER-PLACE-SECRET-KEY-IN-CONFIG-FILE

---
spring:
  config.activate.on-profile: azure

  # TODO-04: Set the spring.application.name to "Lab02 Chat Azure" or something similar.
  # Set spring.main.web-application-type to none to run as a non-web application.
  # Set spring.ai.retry.max-attempts to 1 to fail fast.
  # Set spring.ai.retry.on-client-errors to false since there is typically no point in retrying such a request.
  # Set spring.ai.model.chat to azure-openai to tell SpringAI which autoconfigure class to use.
  # Set spring.ai.azure.openai.endpoint to the value you established during Azure setup.
  # Set spring.ai.azure.openai.chat.options.deployment-name to the value you establised during setup (probably "gpt-35-turbo").
  # Set spring.ai.azure.openai.chat.options.model to the value you established during setup (probably "gpt-35-turbo").

  application.name: Lab02 Chat Azure
  ai:
    model.chat: azure-openai
    azure:
      openai:
        api_key: NEVER-PLACE-SECRET-KEY-IN-CONFIG-FILE
        endpoint: ENDPOINT-GOES-HERE
        chat:
          options:
            deployment-name: DEPLOYMENT-NAME-GOES-HERE
            model: gpt-35-turbo

