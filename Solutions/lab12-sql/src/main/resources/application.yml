spring:
  application.name: Lab11 SQL Generation
  main.web-application-type: none     # Do not start a web server.

  ai:
    retry:
      max-attempts: 1           # Maximum number of retry attempts.
      on-client-errors: false   # If false, throw a NonTransientAiException, and do not attempt retry for 4xx client error codes.

    model.chat: none            # Disable all chat models by default. Models will be explicitly enabled below.

# TODO-02: Adjust the settings below based on the chat model you plan to use.
# Use previous labs for guidance.
#
# If you plan to use Amazon Bedrock: 
#   - Adjust the region setting if needed.
#   - Set spring.ai.model.chat to bedrock-converse to tell SpringAI which autoconfigure class to use.
#   - Makes sure you have enabled the Anthropic Claude 3.5 Sonnet model
#   - Set spring.ai.bedrock.converse.chat.options.model to "anthropic.claude-3-5-sonnet-20240620-v1:0" or equivalent (make sure it is enabled)
#   - Adjust the model if desired or take the default.  See https://docs.aws.amazon.com/bedrock/latest/userguide/models-supported.html for latest list.
#
# If you plan to use OpenAI:
#   - Set spring.ai.model.chat to openai to tell SpringAI which autoconfigure class to use.
#   - Set spring.ai.openai.chat.options.model to "gpt-35-turbo", or see https://platform.openai.com/docs/models/embeddings for latest list.
#
# If you plan to use Azure OpenAI:
#   - Set spring.ai.model.chat to azure-openai to tell SpringAI which autoconfigure class to use.
#   - Set spring.ai.azure.openai.endpoint to the value you established during Azure setup.
#   - Set spring.ai.azure.openai.chat.options.deployment-name to the value you establised during setup.
#   - Set spring.ai.azure.openai.chat.options.model to "gpt-35-turbo", or whichever model you have enabled.
#
# If you plan to use Ollama:
#   - Set spring.ai.model.chat to ollama to tell SpringAI which autoconfigure class to use.
#   - Adjust the base-url if needed.
#   - Make sure Ollama is running.
#
# Note that only one model will be active at a time.  The active model is determined by the active profile.



---
spring:
  config.activate.on-profile: aws

  ai:
    model.chat: bedrock-converse
    bedrock:
      aws.region: us-west-2 # Adjust as needed.
      converse:
        chat:
          options:
            model: anthropic.claude-3-5-sonnet-20240620-v1:0 # Adjust as needed.

---
spring:
  config.activate.on-profile: openai

  ai:
    model.chat: openai
    openai:
      chat:
        options:
          model: gpt-3.5-turbo


---
spring:
  config.activate.on-profile: azure

  ai:
    model.chat: azure-openai
    azure:
      openai:
        endpoint: ENDPOINT-GOES-HERE
        chat:
          options:
            deployment-name: DEPLOYMENT-NAME-GOES-HERE
            model: gpt-35-turbo        

---
spring:
  config.activate.on-profile: ollama

  ai:
    model.chat: ollama
    ollama:
      base-url: http://localhost:11434  # Default base URL when you run Ollama from Docker
