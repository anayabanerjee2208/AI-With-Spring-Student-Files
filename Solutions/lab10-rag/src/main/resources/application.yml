spring:
  application.name: Lab10 RAG
  main.web-application-type: none     # Do not start a web server.

  ai:
    retry:
      max-attempts: 1      # Maximum number of retry attempts.
      on-client-errors: false   # If false, throw a NonTransientAiException, and do not attempt retry for 4xx client error codes.

    openai.embedding.enabled:   false # Allow profile to enable this if needed.
    ollama.embedding.enabled:   false # Allow profile to enable this if needed.

    model.chat: none            # Disable all chat models by default. Models will be explicitly enabled below.

# TODO-03: Adjust the settings below based on the chat model you plan to use.
# Use previous labs for guidance.
#
# If you plan to use Amazon Bedrock: 
#   - Adjust the region setting if needed.
#   - Set spring.ai.model.chat to bedrock-converse to tell SpringAI which autoconfigure class to use.
#   - Set the spring.ai.bedrock.converse.chat.options.model to "anthropic.claude-3-5-sonnet-20240620-v1:0" or equivalent (make sure it is enabled)
#   - Adjust the model if desired or take the default.  See https://docs.aws.amazon.com/bedrock/latest/userguide/models-supported.html for latest list.
#
# If you plan to use OpenAI:
#   - Set spring.ai.model.chat to openai to tell SpringAI which autoconfigure class to use.
#   - Set spring.ai.openai.chat.options.model to "gpt-35-turbo", or see https://platform.openai.com/docs/models/embeddings for latest list.
#
# If you plan to use Azure OpenAI:
#   - Set spring.ai.model.chat to azure-openai to tell SpringAI which autoconfigure class to use.
#   - Set spring.ai.azure.openai.endpoint to the value you established during Azure setup.
#   - Set spring.ai.azure.openai.chat.options.deployment-name to the value you establised during setup.
#   - Set spring.ai.azure.openai.chat.options.model to "gpt-35-turbo", or whichever model you have enabled.
#
# If you plan to use Ollama:
#   - Set spring.ai.model.chat to ollama to tell SpringAI which autoconfigure class to use.
#   - Adjust the base-url if needed.
#   - Make sure Ollama is running.
#
# Note that only one model will be active at a time.  The active model is determined by the active profile.


---
spring:
  config.activate.on-profile: aws

  ai:
    model.chat: bedrock-converse
    bedrock:
      aws.region: us-west-2 # Adjust as needed.
      converse:
        chat:
          options:
            model: anthropic.claude-3-5-sonnet-20240620-v1:0  # Adjust as needed

---
spring:
  config.activate.on-profile: openai

  ai:
    model.chat: openai
    openai:
      api-key: NEVER-PLACE-SECRET-KEY-IN-CONFIG-FILE

---
spring:
  config.activate.on-profile: azure

  ai:
    model.chat: azure-openai
    azure:
      openai:
        api_key: NEVER-PLACE-SECRET-KEY-IN-CONFIG-FILE
        endpoint: ENDPOINT-GOES-HERE
        chat:
          options:
            deployment-name: DEPLOYMENT-NAME-GOES-HERE
            model: gpt-35-turbo

---
spring:
  config.activate.on-profile: ollama

  ai:
    model.chat: ollama
    ollama:
      base-url: http://localhost:11434  # Default base URL when you run Ollama from Docker


---
spring:
  config.activate.on-profile: simple-vector-store
  
  # If running a simple vector store, exclude the Redis, PGVector, and DataSource autoconfigurations.
  autoconfigure.exclude: 
  - org.springframework.ai.autoconfigure.vectorstore.redis.RedisVectorStoreAutoConfiguration
  - org.springframework.ai.autoconfigure.vectorstore.pgvector.PgVectorStoreAutoConfiguration
  - org.springframework.boot.autoconfigure.jdbc.DataSourceAutoConfiguration

---
spring:
  config.activate.on-profile: redis-vector-store

  # TODO-22 (OPTIONAL): Set properties for the Redis vector store.
  # Note that these will only be used if the "redis-vector-store" profile is active.
  # Set spring.ai.vectorstore.redis.uri to redis://localhost:6379, unless your redis is running elsewhere.
  # Set spring.ai.vectorstore.redis.initializeSchema to true to create the necessary schema.
  # (You may not want to do this in a production environment.)
  ai:
    vectorstore:
      redis:
        uri: redis://localhost:6379  # Default.
        index: default-index         # Default.
        prefix: "default:"           # Default.
        initializeSchema: true       # Potentially destructive.

  # If running a redis vector store, exclude the PGVector and DataSource autoconfigurations.
  autoconfigure.exclude: 
  - org.springframework.ai.autoconfigure.vectorstore.pgvector.PgVectorStoreAutoConfiguration
  - org.springframework.boot.autoconfigure.jdbc.DataSourceAutoConfiguration

---
spring:
  config.activate.on-profile: pg-vector-store

  # TODO-25 (OPTIONAL): Set properties for the Postgres vector store.
  # Note that these will only be used if the "pg-vector-store" profile is active.
  # Set spring.datasource.url to jdbc:postgresql://localhost:5432/postgres, unless your Postgres is running elsewhere.
  # Set spring.datasource.username and spring.datasource.password the username and password set when running the container.
  # Set the spring.ai.vectorstore.pgvector.initialize-schema to true to create the necessary schema.
  # (You may not want to do this in a production environment.)
  # Set the spring.ai.vectorstore.pgvector.index-type to HNSW.
  # Set the spring.ai.vectorstore.pgvector.distance-type to COSINE_DISTANCE.
  # Set the spring.ai.vectorstore.pgvector.dimensions based on the the underlying model you are using
  #    For SpringAI's internal implementation, use 384
  #    For AWS/Bedrock/Cohere, use 1024
  #    For OpenAI's text-embedding-ada-002, use 1536
  #    If you make a mistake here, don't worry.  The error message will tell you the correct #.  Delete and restart the DB with the revised number.

  datasource:
    url: jdbc:postgresql://localhost:5432/postgres
    username: postgres
    password: postgres
  ai:
    vectorstore:
      pgvector:
        initialize-schema: true
        index-type: HNSW                # Hierarchical Navigable Small World (HNSW) algorithm, Alternatives: Annoy, LSH, KD-Tree, Ball Tree, Product Quantization, FAISS
        distance-type: COSINE_DISTANCE  # How it determines if one vector is similar to another.
        # CRITICAL: The PGVectorStore database must be initialized to the # of dimensions in the model you are using.
        dimensions: 384                

  # If running a simple vector store, exclude the Redis auto configuration.
  autoconfigure.exclude: 
  - org.springframework.ai.autoconfigure.vectorstore.redis.RedisVectorStoreAutoConfiguration

